# ğŸ’³ Fraud Detection Using Machine Learning (XGBoost)

This project presents a fully modular and production-ready machine learning pipeline designed to detect fraudulent financial transactions using anonymised, real-world payment data. Leveraging **XGBoost** â€” a state-of-the-art gradient boosting algorithm â€” the system prioritises **recall** while maintaining excellent overall accuracy, making it ideal for high-stakes environments like banking and e-commerce.

---

## ğŸ” Project Overview

> Fraud detection is a classification task complicated by severe class imbalance, where fraudulent transactions represent a tiny fraction of total records. The cost of false negatives is high â€” so recall is critical.

This project:
- Uses anonymised real payment data (50,000 records sampled from over 1.2M)
- Builds a modular ML pipeline in Python
- Addresses data imbalance via stratified sampling and threshold tuning
- Trains and tunes an XGBoost model using `GridSearchCV`
- Evaluates with a focus on **recall**, **F1**, and **FÎ²-score**

---

## ğŸ§  Skills Demonstrated

- ğŸ Python programming (Pandas, NumPy, XGBoost, Scikit-learn)
- ğŸ“Š Data preprocessing & anonymisation (SHA-256 hashing)
- ğŸ—ï¸ Modular pipeline design with object-oriented programming
- âš–ï¸ Class imbalance handling (stratified sampling, threshold tuning)
- ğŸ” Hyperparameter tuning & cross-validation (GridSearchCV)
- ğŸ“ˆ Evaluation (confusion matrix, F1, precision/recall, FÎ²-score)
- ğŸ” GDPR awareness (data privacy and minimisation)
- ğŸ§ª Experiment tracking and reproducibility

---

## âš™ï¸ Pipeline Steps Explained

### 1. ğŸ“¦ Data Loading and Initial Cleaning
- Loaded `fraudTrain.csv`, verified existence and format of target variable `is_fraud`
- Normalised labels: values `<1 â†’ 0 (not fraud)`, values `â‰¥1 â†’ 1 (fraud)`

### 2. ğŸ§¼ Preprocessing (via `Preprocessing` class)
- **Anonymisation**: Applied SHA-256 to sensitive columns (e.g., name, dob)
- **Scaling**: StandardScaler to normalise numeric features
- **Encoding**: LabelEncoder used for categorical features (e.g., category, merchant)

### 3. ğŸ§ª Sampling
- Used **stratified sampling** to downsample to 50,000 records
- Ensured class balance consistent with original data (~0.58% fraud)

### 4. ğŸ§  Feature Engineering
- Created derived features:
  - `amt_log`: Log-transformed transaction amount
  - `city_pop_log`: Log-transformed city population
  - `amt_category`: Interaction term between amount and merchant category
- Dropped low-utility features (e.g., cc_num, zip, unix_time)

### 5. ğŸ‹ï¸ Model Training (XGBoost)
- Used `GridSearchCV` to find optimal hyperparameters:
  - `n_estimators: 100â€“150`
  - `max_depth: 5â€“6`
  - `learning_rate: 0.05â€“0.1`
  - `scale_pos_weight: 25â€“50` (to address class imbalance)
- 5-fold cross-validation used during training

### 6. ğŸ¯ Threshold Tuning
- Evaluated model across thresholds `0.2â€“0.5`
- Chose `0.5` as optimal balance point based on FÎ² (Î²=2)

---

## ğŸš€ Results Summary

| Metric        | Value    |
|---------------|----------|
| **Accuracy**  | 99.45%   |
| **Recall**    | 72.41%   |
| **Precision** | 51.85%   |
| **F1-Score**  | 0.6043   |
| **FÎ²-Score**  | 0.6709   |

ğŸ“Œ **At threshold 0.5**, the model achieved high recall with tolerable false positives â€” ideal for catching as much fraud as possible without overwhelming alerts.

ğŸ§® **Confusion Matrix**:
- TP: 42
- FN: 16
- FP: 39
- TN: 9903

---

## ğŸ—‚ï¸ Repository Structure

```bash
ğŸ“ vision_base.py       # Main pipeline controller (orchestration script)
ğŸ“ vision_mod.py        # Preprocessing class: scaling, encoding, anonymisation
ğŸ“ vision_train.py      # Training, tuning, threshold evaluation
ğŸ“ report.pdf           # Full technical dissertation/report (IEEE style)
ğŸ“ poster.jpg           # A3 visual poster (showcase ready)
ğŸ“ screenshot/          # Screenshots of dataset (not uploaded due to file size)
ğŸ“ README.md            # This file â€“ public-facing project summary
